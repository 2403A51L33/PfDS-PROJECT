{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6OKw5kB8lXs/ZE2s2xymr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2403A51L33/PfDS-PROJECT/blob/main/DEEP%20LEARNING%20ALGORITHMS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnoK0BllhZBW"
      },
      "outputs": [],
      "source": [
        "import os, re, json, math, random, argparse, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_OK = True\n",
        "except Exception:\n",
        "    SHAP_OK = False\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "def get_args():\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--csv\", type=str, default=\"/mnt/data/realistic_drug_labels_side_effects.csv\")\n",
        "    p.add_argument(\"--model\", type=str, default=\"fusion\",\n",
        "                   choices=[\"mlp_tab\",\"text_cnn\",\"bilstm_attn\",\"transformer\",\"tab_transformer\",\"fusion\"])\n",
        "    p.add_argument(\"--epochs\", type=int, default=5)\n",
        "    p.add_argument(\"--batch_size\", type=int, default=64)\n",
        "    p.add_argument(\"--max_vocab\", type=int, default=30000)\n",
        "    p.add_argument(\"--max_len\", type=int, default=256)\n",
        "    p.add_argument(\"--embed_dim\", type=int, default=128)\n",
        "    p.add_argument(\"--hidden\", type=int, default=256)\n",
        "    p.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    p.add_argument(\"--weight_decay\", type=float, default=1e-4)\n",
        "    p.add_argument(\"--dropout\", type=float, default=0.2)\n",
        "    p.add_argument(\"--text_encoder\", type=str, default=\"transformer\",\n",
        "                   choices=[\"transformer\",\"bilstm_attn\",\"text_cnn\"])  # used by fusion\n",
        "    p.add_argument(\"--no_shap\", action=\"store_true\", help=\"Skip SHAP even if available\")\n",
        "    return p.parse_args([]) if \"_file_\" not in globals() else p.parse_args()\n",
        "\n",
        "ARGS = get_args()\n",
        "OUTDIR = \"./dl_outputs\"; os.makedirs(OUTDIR, exist_ok=True)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if not os.path.exists(ARGS.csv):\n",
        "    alt = \"./realistic_drug_labels_side_effects.csv\"\n",
        "    if os.path.exists(alt):\n",
        "        ARGS.csv = alt\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"CSV not found at {ARGS.csv} or {alt}\")\n",
        "\n",
        "df = pd.read_csv(ARGS.csv)\n",
        "\n",
        "TEXT_COLS = [c for c in [\"indications\",\"side_effects\",\"contraindications\",\"warnings\"] if c in df.columns]\n",
        "NUM_COLS  = [c for c in [\"dosage_mg\",\"price_usd\",\"approval_year\"] if c in df.columns]\n",
        "CAT_COLS  = [c for c in [\"drug_class\",\"administration_route\",\"approval_status\",\"manufacturer\"] if c in df.columns]\n",
        "\n",
        "for c in TEXT_COLS: df[c] = df[c].fillna(\"\")\n",
        "for c in NUM_COLS:  df[c] = df[c].astype(float)\n",
        "for c in CAT_COLS:  df[c] = df[c].astype(str).fillna(\"UNK\")\n",
        "\n",
        "def _to_num(v):\n",
        "    try: return float(v)\n",
        "    except: return {\"low\":0,\"mild\":0,\"moderate\":1,\"medium\":1,\"high\":2,\"severe\":2}.get(str(v).lower().strip(), np.nan)\n",
        "\n",
        "y_raw = df[\"side_effect_severity\"]\n",
        "if y_raw.dtype.kind in \"ifu\":\n",
        "    q = np.quantile(y_raw, [0.33,0.66]);\n",
        "    y = y_raw.apply(lambda v: 0 if v<=q[0] else (1 if v<=q[1] else 2)).astype(int).values\n",
        "else:\n",
        "    tmp = y_raw.apply(_to_num)\n",
        "    if tmp.isna().mean()<0.5:\n",
        "        q = np.quantile(tmp.fillna(tmp.median()), [0.33,0.66])\n",
        "        y = tmp.fillna(tmp.median()).apply(lambda v: 0 if v<=q[0] else (1 if v<=q[1] else 2)).astype(int).values\n",
        "    else:\n",
        "        cats = {k:i for i,k in enumerate(sorted(y_raw.astype(str).unique()))}\n",
        "        y = y_raw.astype(str).map(cats).values % 3  # crude\n",
        "NUM_CLASSES = 3\n",
        "CLASS_NAMES = [\"low\",\"moderate\",\"high\"]\n",
        "\n",
        "def simple_tokenize(s):\n",
        "    s = re.sub(r\"[^A-Za-z0-9\\s\\-\\_/\\.]\", \" \", s.lower())\n",
        "    return [t for t in s.split() if t]\n",
        "\n",
        "full_text = (df[TEXT_COLS].apply(lambda r: \" \".join(map(str, r.values)), axis=1)\n",
        "             if TEXT_COLS else pd.Series([\"\"]*len(df)))\n",
        "tokens = [simple_tokenize(t) for t in full_text]\n",
        "\n",
        "freq = Counter([w for ts in tokens for w in ts])\n",
        "most = [w for w,_ in freq.most_common(ARGS.max_vocab-2)]\n",
        "itos = [\"<pad>\",\"<unk>\"] + most\n",
        "stoi = {w:i for i,w in enumerate(itos)}\n",
        "\n",
        "def encode(ts, max_len=ARGS.max_len):\n",
        "    ids = [stoi.get(w,1) for w in ts][:max_len]\n",
        "    if len(ids)<max_len: ids += [0]*(max_len-len(ids))\n",
        "    return np.array(ids, dtype=np.int64)\n",
        "\n",
        "X_text_ids = np.vstack([encode(ts) for ts in tokens]) if TEXT_COLS else np.zeros((len(df), ARGS.max_len), dtype=np.int64)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_num = scaler.fit_transform(df[NUM_COLS].values) if NUM_COLS else np.zeros((len(df),0),dtype=np.float32)\n",
        "\n",
        "cat_maps = []\n",
        "cat_card = []\n",
        "X_cat_ids = []\n",
        "for c in CAT_COLS:\n",
        "    vals = df[c].astype(str).values\n",
        "    uniq = [\"<unk>\"] + sorted(list(set(vals)))\n",
        "    m = {u:i for i,u in enumerate(uniq)}\n",
        "    cat_maps.append(m)\n",
        "    cat_card.append(len(uniq))\n",
        "    X_cat_ids.append(np.array([m.get(v,0) for v in vals], dtype=np.int64))\n",
        "X_cat_ids = np.stack(X_cat_ids, axis=1) if CAT_COLS else np.zeros((len(df),0), dtype=np.int64)\n",
        "\n",
        "X_train_idx, X_test_idx = train_test_split(np.arange(len(df)), test_size=0.2, random_state=SEED, stratify=y)\n",
        "def split_arr(arr): return arr[X_train_idx], arr[X_test_idx]\n",
        "Xtr_text, Xte_text = split_arr(X_text_ids)\n",
        "Xtr_num,  Xte_num  = split_arr(X_num)\n",
        "Xtr_cat,  Xte_cat  = split_arr(X_cat_ids)\n",
        "y_train,  y_test   = y[X_train_idx], y[X_test_idx]\n",
        "\n",
        "class_counts = np.bincount(y_train, minlength=NUM_CLASSES)\n",
        "weights = (class_counts.sum() / (class_counts + 1e-6))\n",
        "weights = weights / weights.mean()\n",
        "CLASS_WEIGHTS = torch.tensor(weights, dtype=torch.float32, device=DEVICE)\n",
        "\n",
        "class DS(torch.utils.data.Dataset):\n",
        "    def __init__(self, text_ids, num, cat, y):\n",
        "        self.text_ids = text_ids; self.num = num; self.cat = cat; self.y = y\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, i):\n",
        "        return (torch.tensor(self.text_ids[i],dtype=torch.long),\n",
        "                torch.tensor(self.num[i],dtype=torch.float32),\n",
        "                torch.tensor(self.cat[i],dtype=torch.long),\n",
        "                torch.tensor(self.y[i],dtype=torch.long))\n",
        "tr_loader = torch.utils.data.DataLoader(DS(Xtr_text,Xtr_num,Xtr_cat,y_train), batch_size=ARGS.batch_size, shuffle=True)\n",
        "te_loader = torch.utils.data.DataLoader(DS(Xte_text,Xte_num,Xte_cat,y_test),  batch_size=ARGS.batch_size, shuffle=False)\n",
        "\n",
        "class TabularMLP(nn.Module):\n",
        "    def __init__(self, num_dim, cat_cards, emb_dim=32, hidden=256, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.cat_embs = nn.ModuleList([nn.Embedding(c, min(emb_dim, max(4,int(round(c**0.25))))) for c in cat_cards])\n",
        "        cat_out = sum(e.embedding_dim for e in self.cat_embs)\n",
        "        in_dim = num_dim + cat_out\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden), nn.ReLU(), nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, hidden), nn.ReLU(), nn.Dropout(dropout)\n",
        "        )\n",
        "        self.out_dim = hidden\n",
        "    def forward(self, x_num, x_cat):\n",
        "        if len(self.cat_embs):\n",
        "            embs = [emb(x_cat[:,i]) for i,emb in enumerate(self.cat_embs)]\n",
        "            cat_e = torch.cat(embs, dim=1)\n",
        "        else:\n",
        "            cat_e = torch.zeros(x_num.size(0),0,device=x_num.device)\n",
        "        x = torch.cat([x_num, cat_e], dim=1)\n",
        "        return self.mlp(x)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0,max_len).unsqueeze(1).float()\n",
        "        div = torch.exp(torch.arange(0,d_model,2).float() * (-math.log(10000.0)/d_model))\n",
        "        pe[:,0::2] = torch.sin(pos*div); pe[:,1::2] = torch.cos(pos*div)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0)) # (1,L,D)\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:,:x.size(1)]\n",
        "\n",
        "class TextCNN(nn.Module):\n",
        "    def __init__(self, vocab, d_model=128, num_classes=3, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab, d_model, padding_idx=0)\n",
        "        self.convs = nn.ModuleList([nn.Conv1d(d_model, d_model, k) for k in [3,4,5]])\n",
        "        self.fc = nn.Sequential(nn.Linear(d_model*3, d_model), nn.ReLU(), nn.Dropout(dropout))\n",
        "        self.head = nn.Linear(d_model, num_classes)\n",
        "    def forward(self, x_ids):\n",
        "        x = self.emb(x_ids)\n",
        "        x = x.transpose(1,2)\n",
        "        feats = [F.max_pool1d(F.relu(conv(x)), kernel_size=conv(x).size(-1)).squeeze(-1) for conv in self.convs]\n",
        "        h = torch.cat(feats, dim=1)\n",
        "        h = self.fc(h)\n",
        "        return h, self.head(h)\n",
        "\n",
        "class BiLSTMAttn(nn.Module):\n",
        "    def __init__(self, vocab, d_model=128, hidden=128, num_classes=3, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab, d_model, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(d_model, hidden, num_layers=1, batch_first=True, bidirectional=True)\n",
        "        self.attn = nn.Linear(2*hidden, 1)\n",
        "        self.fc   = nn.Sequential(nn.Linear(2*hidden, hidden), nn.ReLU(), nn.Dropout(dropout))\n",
        "        self.head = nn.Linear(hidden, num_classes)\n",
        "    def forward(self, x_ids):\n",
        "        x = self.emb(x_ids)\n",
        "        h, _ = self.lstm(x)\n",
        "        a = torch.softmax(self.attn(h).squeeze(-1), dim=1)\n",
        "        ctx = (h * a.unsqueeze(-1)).sum(1)\n",
        "        z = self.fc(ctx)\n",
        "        return z, self.head(z)\n",
        "\n",
        "class SimpleTransformer(nn.Module):\n",
        "    def __init__(self, vocab, d_model=128, nhead=4, num_layers=2, num_classes=3, dropout=0.2, max_len=2048):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab, d_model, padding_idx=0)\n",
        "        self.pos = PositionalEncoding(d_model, max_len)\n",
        "        layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=4*d_model, dropout=dropout, batch_first=True)\n",
        "        self.enc = nn.TransformerEncoder(layer, num_layers=num_layers)\n",
        "        self.fc  = nn.Sequential(nn.Linear(d_model, d_model), nn.ReLU(), nn.Dropout(dropout))\n",
        "        self.head= nn.Linear(d_model, num_classes)\n",
        "    def forward(self, x_ids):\n",
        "        x = self.emb(x_ids)\n",
        "        x = self.pos(x)\n",
        "        mask = (x_ids==0)\n",
        "        h = self.enc(x, src_key_padding_mask=mask)\n",
        "        pooled = (h.masked_fill(mask.unsqueeze(-1), 0).sum(1) /\n",
        "                  (~mask).sum(1).clamp(min=1).unsqueeze(-1))\n",
        "        z = self.fc(pooled)\n",
        "        return z, self.head(z)\n",
        "\n",
        "class TabTransformer(nn.Module):\n",
        "    def __init__(self, cat_cards, num_dim, d_model=128, nhead=4, num_layers=2, num_classes=3, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.embs = nn.ModuleList([nn.Embedding(c, d_model) for c in cat_cards])\n",
        "        layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=4*d_model, dropout=dropout, batch_first=True)\n",
        "        self.enc = nn.TransformerEncoder(layer, num_layers=num_layers)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(d_model*len(cat_cards)+num_dim, 256), nn.ReLU(), nn.Dropout(dropout),\n",
        "            nn.Linear(256, 256), nn.ReLU(), nn.Dropout(dropout))\n",
        "        self.head = nn.Linear(256, num_classes)\n",
        "    def forward(self, x_num, x_cat):\n",
        "        if len(self.embs):\n",
        "            toks = torch.stack([emb(x_cat[:,i]) for i,emb in enumerate(self.embs)], dim=1) # B,C,D\n",
        "            h = self.enc(toks)                                      # B,C,D\n",
        "            h = h.reshape(h.size(0), -1)                           # B, C*D\n",
        "        else:\n",
        "            h = torch.zeros(x_num.size(0),0,device=x_num.device)\n",
        "        z = torch.cat([h, x_num], dim=1)\n",
        "        z = self.mlp(z)\n",
        "        return z, self.head(z)\n",
        "\n",
        "class FusionNet(nn.Module):\n",
        "    def __init__(self, vocab, num_dim, cat_cards, text_encoder=\"transformer\", d_model=128, hidden=256, num_classes=3, dropout=0.2):\n",
        "        super().__init__()\n",
        "        if text_encoder == \"transformer\":\n",
        "            self.text = SimpleTransformer(vocab, d_model=d_model, nhead=4, num_layers=2, num_classes=num_classes, dropout=dropout)\n",
        "        elif text_encoder == \"bilstm_attn\":\n",
        "            self.text = BiLSTMAttn(vocab, d_model=d_model, hidden=d_model, num_classes=num_classes, dropout=dropout)\n",
        "        else:\n",
        "            self.text = TextCNN(vocab, d_model=d_model, num_classes=num_classes, dropout=dropout)\n",
        "        self.tab = TabularMLP(num_dim=num_dim, cat_cards=cat_cards, emb_dim=32, hidden=hidden, dropout=dropout)\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Linear(d_model + self.tab.out_dim, hidden), nn.ReLU(), nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, hidden), nn.ReLU(), nn.Dropout(dropout)\n",
        "        )\n",
        "        self.head = nn.Linear(hidden, num_classes)\n",
        "    def forward(self, x_ids, x_num, x_cat):\n",
        "        tz, _ = self.text(x_ids)\n",
        "        sz = self.tab(x_num, x_cat)\n",
        "        z = torch.cat([tz, sz], dim=1)\n",
        "        z = self.fuse(z)\n",
        "        return z, self.head(z)\n",
        "\n",
        "VOCAB_SIZE = len(itos)\n",
        "NUM_DIM = Xtr_num.shape[1]\n",
        "CAT_CARDS = cat_card\n",
        "\n",
        "def make_model(name):\n",
        "    if name==\"mlp_tab\":\n",
        "        class MLPWrap(nn.Module):\n",
        "            def __init__(self):\n",
        "                super().__init__()\n",
        "                self.base = TabularMLP(NUM_DIM, CAT_CARDS, emb_dim=32, hidden=ARGS.hidden, dropout=ARGS.dropout)\n",
        "                self.head=nn.Linear(self.base.out_dim, NUM_CLASSES)\n",
        "            def forward(self, ids, num, cat):\n",
        "                z = self.base(num, cat)\n",
        "                return z, self.head(z)\n",
        "        return MLPWrap()\n",
        "    if name==\"text_cnn\":      return TextCNN(VOCAB_SIZE, d_model=ARGS.embed_dim, num_classes=NUM_CLASSES, dropout=ARGS.dropout)\n",
        "    if name==\"bilstm_attn\":   return BiLSTMAttn(VOCAB_SIZE, d_model=ARGS.embed_dim, hidden=ARGS.embed_dim, num_classes=NUM_CLASSES, dropout=ARGS.dropout)\n",
        "    if name==\"transformer\":   return SimpleTransformer(VOCAB_SIZE, d_model=ARGS.embed_dim, nhead=4, num_layers=2, num_classes=NUM_CLASSES, dropout=ARGS.dropout, max_len=ARGS.max_len)\n",
        "    if name==\"tab_transformer\":\n",
        "        class TabWrap(nn.Module):\n",
        "            def __init__(self):\n",
        "                super().__init__()\n",
        "                self.base = TabTransformer(CAT_CARDS, NUM_DIM, d_model=ARGS.embed_dim, nhead=4, num_layers=2, num_classes=NUM_CLASSES, dropout=ARGS.dropout)\n",
        "                self.head = nn.Linear(self.base.mlp[3].out_features, NUM_CLASSES)\n",
        "            def forward(self, ids, num, cat):\n",
        "                z, logits = self.base(num, cat)\n",
        "                return z, logits\n",
        "        return TabWrap()\n",
        "    if name==\"fusion\":\n",
        "        class FusWrap(nn.Module):\n",
        "            def __init__(self):\n",
        "                super().__init__()\n",
        "                self.base = FusionNet(VOCAB_SIZE, NUM_DIM, CAT_CARDS, text_encoder=ARGS.text_encoder, d_model=ARGS.embed_dim, hidden=ARGS.hidden, num_classes=NUM_CLASSES, dropout=ARGS.dropout)\n",
        "            def forward(self, ids, num, cat):\n",
        "                return self.base(ids, num, cat)\n",
        "        return FusWrap()\n",
        "    raise ValueError(\"Unknown model\")\n",
        "\n",
        "model = make_model(ARGS.model).to(DEVICE)\n",
        "optim_ = optim.AdamW(model.parameters(), lr=ARGS.lr, weight_decay=ARGS.weight_decay)\n",
        "criterion = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS)\n",
        "\n",
        "def step_batch(batch, train=True):\n",
        "    ids, num, cat, yy = [b.to(DEVICE) for b in batch]\n",
        "    if ARGS.model in [\"text_cnn\",\"bilstm_attn\",\"transformer\"]:\n",
        "        z, logits = model(ids)\n",
        "    elif ARGS.model in [\"tab_transformer\",\"mlp_tab\"]:\n",
        "        z, logits = model(ids, num, cat)\n",
        "    else:  # fusion\n",
        "        z, logits = model(ids, num, cat)\n",
        "    loss = criterion(logits, yy)\n",
        "    if train:\n",
        "        optim_.zero_grad(); loss.backward(); optim_.step()\n",
        "    return loss.item(), logits.detach(), yy.detach()\n",
        "\n",
        "def run_epoch(loader, train=True):\n",
        "    model.train(mode=train)\n",
        "    losses = []; all_logits=[]; all_y=[]\n",
        "    for batch in loader:\n",
        "        l, lo, yy = step_batch(batch, train=train)\n",
        "        losses.append(l)\n",
        "        all_logits.append(lo.cpu()); all_y.append(yy.cpu())\n",
        "    logits = torch.cat(all_logits); ytrue = torch.cat(all_y)\n",
        "    ypred = logits.argmax(1).numpy()\n",
        "    report = classification_report(ytrue, ypred, target_names=CLASS_NAMES, output_dict=True, zero_division=0)\n",
        "    cm = confusion_matrix(ytrue, ypred, labels=list(range(NUM_CLASSES))).tolist()\n",
        "    return float(np.mean(losses)), report, cm, logits.numpy(), ytrue.numpy()\n",
        "\n",
        "best_f1 = -1.0\n",
        "for ep in range(1, ARGS.epochs+1):\n",
        "    tr_loss, tr_rep, tr_cm, _, _ = run_epoch(tr_loader, train=True)\n",
        "    te_loss, te_rep, te_cm, te_logits, te_y = run_epoch(te_loader, train=False)\n",
        "    macro_f1 = te_rep[\"macro avg\"][\"f1-score\"]\n",
        "    print(f\"Epoch {ep:02d} | train loss {tr_loss:.4f} | test loss {te_loss:.4f} | macroF1 {macro_f1:.3f}\")\n",
        "    if macro_f1 > best_f1:\n",
        "        best_f1 = macro_f1\n",
        "        torch.save(model.state_dict(), os.path.join(OUTDIR, f\"model_{ARGS.model}.pt\"))\n",
        "        with open(os.path.join(OUTDIR, f\"metrics_{ARGS.model}.json\"), \"w\") as f:\n",
        "            json.dump({\"train\":tr_rep,\"test\":te_rep,\"confusion_matrix_test\":te_cm}, f, indent=2)\n",
        "        np.save(os.path.join(OUTDIR, f\"probs_{ARGS.model}.npy\"), F.softmax(torch.tensor(te_logits),dim=1).numpy())\n",
        "\n",
        "def compute_saliency_text(ids_batch):\n",
        "    ids = torch.tensor(ids_batch[:64], dtype=torch.long, device=DEVICE)\n",
        "    if ARGS.model in [\"text_cnn\",\"bilstm_attn\",\"transformer\"]:\n",
        "        emb_layer = model.emb if hasattr(model, \"emb\") else model.text.emb\n",
        "        emb = emb_layer(ids); emb.retain_grad()\n",
        "        if ARGS.model==\"text_cnn\":\n",
        "            x = emb.transpose(1,2)\n",
        "            feats = [F.relu(conv(x)) for conv in model.convs]\n",
        "            pooled = [F.max_pool1d(f, f.size(-1)).squeeze(-1) for f in feats]\n",
        "            h = torch.cat(pooled, dim=1)\n",
        "            z = model.fc(h); logits = model.head(z)\n",
        "        elif ARGS.model==\"bilstm_attn\":\n",
        "            h,_ = model.lstm(emb); a = torch.softmax(model.attn(h).squeeze(-1), dim=1); ctx=(h*a.unsqueeze(-1)).sum(1)\n",
        "            z = model.fc(ctx); logits = model.head(z)\n",
        "        else:\n",
        "            x = model.pos(emb); mask = (ids==0); h = model.enc(x, src_key_padding_mask=mask)\n",
        "            pooled = (h.masked_fill(mask.unsqueeze(-1), 0).sum(1) / (~mask).sum(1).clamp(min=1).unsqueeze(-1))\n",
        "            z = model.fc(pooled); logits = model.head(z)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        top = probs.max(1).values.sum()\n",
        "        top.backward()\n",
        "        sal = (emb.grad*emb).abs().sum(-1).detach().cpu().numpy()  # B,L\n",
        "        return sal\n",
        "    return None\n",
        "\n",
        "def compute_saliency_tab(num_batch):\n",
        "    num = torch.tensor(num_batch[:128], dtype=torch.float32, device=DEVICE, requires_grad=True)\n",
        "    ids_dummy = torch.zeros((num.size(0), ARGS.max_len), dtype=torch.long, device=DEVICE)\n",
        "    cat_dummy = torch.zeros((num.size(0), Xtr_cat.shape[1]), dtype=torch.long, device=DEVICE)\n",
        "    if ARGS.model in [\"mlp_tab\",\"tab_transformer\"]:\n",
        "        z, logits = model(ids_dummy, num, cat_dummy)\n",
        "    else:\n",
        "        z, logits = model(ids_dummy, num, cat_dummy)\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    top = probs.max(1).values.sum()\n",
        "    top.backward()\n",
        "    sal = (num.grad * num).abs().detach().cpu().numpy()\n",
        "    return sal\n",
        "\n",
        "try:\n",
        "    ids_batch = Xte_text[:64]\n",
        "    num_batch = Xte_num[:128] if NUM_DIM>0 else None\n",
        "    s_text = compute_saliency_text(ids_batch) if TEXT_COLS else None\n",
        "    s_tab  = compute_saliency_tab(num_batch) if (ARGS.model in [\"mlp_tab\",\"tab_transformer\",\"fusion\"] and NUM_DIM>0) else None\n",
        "    np.savez(os.path.join(OUTDIR, f\"saliency_{ARGS.model}.npz\"), text=s_text, tab=s_tab)\n",
        "except Exception as e:\n",
        "    print(\"Saliency error:\", e)\n",
        "\n",
        "if SHAP_OK and not ARGS.no_shap:\n",
        "    try:\n",
        "        model.eval()\n",
        "        def pred_fn(X_concat):\n",
        "            B = X_concat.shape[0]\n",
        "            ids = torch.tensor(Xte_text[:B], dtype=torch.long, device=DEVICE)\n",
        "            num = torch.tensor(Xte_num[:B], dtype=torch.float32, device=DEVICE)\n",
        "            cat = torch.tensor(Xte_cat[:B], dtype=torch.long, device=DEVICE)\n",
        "            with torch.no_grad():\n",
        "                if ARGS.model in [\"text_cnn\",\"bilstm_attn\",\"transformer\"]:\n",
        "                    _, logits = model(ids)\n",
        "                elif ARGS.model in [\"tab_transformer\",\"mlp_tab\"]:\n",
        "                    _, logits = model(ids, num, cat)\n",
        "                else:\n",
        "                    _, logits = model(ids, num, cat)\n",
        "                return F.softmax(logits, dim=1).detach().cpu().numpy()\n",
        "        bg = np.zeros((30, 10))  # dummy background; we rely on model internals for actual inputs\n",
        "        expl = shap.KernelExplainer(pred_fn, bg)\n",
        "        sample = np.zeros((20, 10))\n",
        "        vals = expl.shap_values(sample, nsamples=100)\n",
        "        np.save(os.path.join(OUTDIR, f\"shap_values_{ARGS.model}.npy\"), vals, allow_pickle=True)\n",
        "    except Exception as e:\n",
        "        print(\"SHAP error:\", e)\n",
        "\n",
        "print(\"Done. Artifacts in:\", OUTDIR)"
      ]
    }
  ]
}