{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPL4zdWZ1OWaTz4QWErglnP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2403A51L33/PfDS-PROJECT/blob/main/FULL%20CODE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEsO4mdCq1MU",
        "outputId": "81b201da-0957-43ae-bd07-4045316e78fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found CSV at specified path: /content/realistic_drug_labels_side_effects.csv\n",
            "Dataset shape: (1436, 15)\n",
            "Numeric cols: ['approval_year', 'dosage_mg', 'price_usd']\n",
            "Categorical cols: ['drug_name', 'manufacturer', 'drug_class', 'indications', 'side_effects', 'administration_route', 'contraindications', 'warnings', 'batch_number', 'expiry_date', 'approval_status']\n",
            "\n",
            "Training stacked ensemble model...\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
            "\n",
            "Classification Report (Test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3942    0.4184    0.4059        98\n",
            "           1     0.3176    0.2784    0.2967        97\n",
            "           2     0.2727    0.2903    0.2812        93\n",
            "\n",
            "    accuracy                         0.3299       288\n",
            "   macro avg     0.3282    0.3290    0.3280       288\n",
            "weighted avg     0.3292    0.3299    0.3289       288\n",
            "\n",
            "Final Test Accuracy: 0.3299\n",
            "Final Test F1 (macro): 0.3280\n",
            "\n",
            "Baseline GradientBoosting F1 ≈ 0.3419 → Check if improved!\n",
            "\n",
            "Saved model to models/stacked_pipeline_fixed.joblib\n",
            "\n",
            "Computing SHAP explainability (tree-based model)...\n",
            "SHAP skipped due to error: Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. Consider retrying with the feature_perturbation='interventional' option. This check failed because for one of the samples the sum of the SHAP values was 1065009707820994782433051148288.000000, while the model output was 0.292500. If this difference is acceptable you can set check_additivity=False to disable this check.\n",
            "\n",
            "✅ Script finished successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import glob\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    HAS_XGB = True\n",
        "except Exception:\n",
        "    HAS_XGB = False\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    HAS_LGB = True\n",
        "except Exception:\n",
        "    HAS_LGB = False\n",
        "\n",
        "import shap\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.2\n",
        "\n",
        "DATASET_PATH = \"/content/realistic_drug_labels_side_effects.csv\"\n",
        "\n",
        "def find_csv_in_zip(zip_path):\n",
        "    if not os.path.exists(zip_path):\n",
        "        return None\n",
        "    extract_dir = os.path.splitext(zip_path)[0] + \"_extracted\"\n",
        "    os.makedirs(extract_dir, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "        z.extractall(extract_dir)\n",
        "    csvs = glob.glob(os.path.join(extract_dir, \"**\", \"*.csv\"), recursive=True)\n",
        "    return csvs[0] if csvs else None\n",
        "\n",
        "def find_first_csv_in_dir(search_dir=\"/mnt/data\"):\n",
        "    csvs = glob.glob(os.path.join(search_dir, \"**\", \"*.csv\"), recursive=True)\n",
        "    return csvs[0] if csvs else None\n",
        "\n",
        "def load_dataset():\n",
        "    if os.path.exists(DATASET_PATH):\n",
        "        print(\"Found CSV at specified path:\", DATASET_PATH)\n",
        "        return pd.read_csv(DATASET_PATH)\n",
        "    raise FileNotFoundError(\"No CSV found. Place dataset in /mnt/data or set DATASET_PATH correctly.\")\n",
        "\n",
        "df = load_dataset()\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "\n",
        "TARGET_COL = \"side_effect_severity\"\n",
        "if TARGET_COL not in df.columns:\n",
        "    for alt in [\"approval_status\", \"target\", \"label\"]:\n",
        "        if alt in df.columns:\n",
        "            TARGET_COL = alt\n",
        "            print(\"Using alternate target:\", TARGET_COL)\n",
        "            break\n",
        "    else:\n",
        "        raise ValueError(f\"Target column '{TARGET_COL}' not found. Columns: {df.columns.tolist()}\")\n",
        "\n",
        "X = df.drop(columns=[TARGET_COL])\n",
        "y = df[TARGET_COL].copy().astype(\"category\").cat.codes\n",
        "\n",
        "numeric_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
        "\n",
        "for col in numeric_cols[:]:\n",
        "    if X[col].nunique() <= 20:\n",
        "        numeric_cols.remove(col)\n",
        "        categorical_cols.append(col)\n",
        "\n",
        "print(\"Numeric cols:\", numeric_cols)\n",
        "print(\"Categorical cols:\", categorical_cols)\n",
        "\n",
        "numeric_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", numeric_transformer, numeric_cols),\n",
        "    (\"cat\", categorical_transformer, categorical_cols)\n",
        "], remainder=\"drop\")\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=400, max_depth=None, class_weight=\"balanced\",\n",
        "    n_jobs=-1, random_state=RANDOM_STATE\n",
        ")\n",
        "estimators = [(\"rf\", rf)]\n",
        "\n",
        "if HAS_XGB:\n",
        "    xgb_c = xgb.XGBClassifier(\n",
        "        n_estimators=400, learning_rate=0.05, max_depth=6,\n",
        "        subsample=0.9, colsample_bytree=0.8, use_label_encoder=False,\n",
        "        eval_metric=\"mlogloss\", random_state=RANDOM_STATE\n",
        "    )\n",
        "    estimators.append((\"xgb\", xgb_c))\n",
        "\n",
        "if HAS_LGB:\n",
        "    lgb_c = lgb.LGBMClassifier(\n",
        "        n_estimators=400, learning_rate=0.05, num_leaves=63,\n",
        "        feature_fraction=0.9, bagging_fraction=0.8,\n",
        "        random_state=RANDOM_STATE, n_jobs=-1\n",
        "    )\n",
        "    estimators.append((\"lgbm\", lgb_c))\n",
        "\n",
        "if len(estimators) < 2:\n",
        "    estimators.append((\"rf2\", RandomForestClassifier(\n",
        "        n_estimators=300, random_state=RANDOM_STATE+1, class_weight=\"balanced\"\n",
        "    )))\n",
        "\n",
        "meta = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
        "\n",
        "stack_model = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=meta,\n",
        "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
        "    n_jobs=-1,\n",
        "    passthrough=False\n",
        ")\n",
        "\n",
        "full_pipeline = ImbPipeline([\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"smote\", SMOTE(random_state=RANDOM_STATE)),\n",
        "    (\"stack\", stack_model)\n",
        "])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, stratify=y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "print(\"\\nTraining stacked ensemble model...\")\n",
        "full_pipeline.fit(X_train, y_train)\n",
        "\n",
        "y_pred = full_pipeline.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "f1m = f1_score(y_test, y_pred, average=\"macro\")\n",
        "\n",
        "print(\"\\nClassification Report (Test):\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "print(f\"Final Test Accuracy: {acc:.4f}\")\n",
        "print(f\"Final Test F1 (macro): {f1m:.4f}\")\n",
        "print(\"\\nBaseline GradientBoosting F1 ≈ 0.3419 → Check if improved!\")\n",
        "\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "joblib.dump(full_pipeline, \"models/stacked_pipeline_fixed.joblib\")\n",
        "print(\"\\nSaved model to models/stacked_pipeline_fixed.joblib\")\n",
        "\n",
        "print(\"\\nComputing SHAP explainability (tree-based model)...\")\n",
        "try:\n",
        "    rf_model = full_pipeline.named_steps[\"stack\"].estimators_[0]\n",
        "    X_train_proc = full_pipeline.named_steps[\"preprocessor\"].transform(X_train)\n",
        "    background = shap.utils.sample(X_train_proc, 100)\n",
        "    explainer = shap.TreeExplainer(rf_model)\n",
        "    X_test_proc = full_pipeline.named_steps[\"preprocessor\"].transform(X_test)\n",
        "    sample = shap.utils.sample(X_test_proc, min(50, X_test_proc.shape[0]))\n",
        "    shap_values = explainer.shap_values(sample)\n",
        "    shap.summary_plot(shap_values, sample, show=False)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"shap_summary.png\", bbox_inches=\"tight\")\n",
        "    print(\"Saved SHAP summary plot → shap_summary.png\")\n",
        "except Exception as e:\n",
        "    print(\"SHAP skipped due to error:\", e)\n",
        "\n",
        "print(\"\\n Script finished successfully.\")"
      ]
    }
  ]
}