{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/v8YhKAXe2vBIDVk11yOp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2403A51L33/PfDS-PROJECT/blob/main/REINFORCEMENT%20LEARNING(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WN_CAe6KkXwu",
        "outputId": "b2aec495-6472-4461-f317-6e26b87ff186"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Placeholder for train_non_contextual_bandits\n",
            "Placeholder for train_contextual_bandits\n",
            "Placeholder for train_deep_rl\n",
            "Placeholder for train_offline_rl\n",
            "Placeholder for explain_policy_with_shap\n",
            "Placeholder for distill_policy_to_tree\n",
            "Done. Artifacts in: ./rl_outputs\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier, export_text\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "except Exception:\n",
        "    SHAP_AVAILABLE = False\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "OUTDIR = \"./rl_outputs\"\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "\n",
        "DATA_PATH = \"/content/realistic_drug_labels_side_effects.csv\"\n",
        "\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    alt = \"./realistic_drug_labels_side_effects.csv\"\n",
        "    if os.path.exists(alt):\n",
        "        DATA_PATH = alt\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Dataset not found at {DATA_PATH} or {alt}\")\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "def to_three_bins(x):\n",
        "    try:\n",
        "        val = float(x)\n",
        "        return val\n",
        "    except:\n",
        "        s = str(x).strip().lower()\n",
        "        mapping = {\"low\":0, \"mild\":0, \"moderate\":1, \"medium\":1, \"high\":2, \"severe\":2}\n",
        "        return mapping.get(s, 1)\n",
        "\n",
        "if pd.api.types.is_numeric_dtype(df[\"side_effect_severity\"]):\n",
        "    q = np.quantile(df[\"side_effect_severity\"], [0.33, 0.66])\n",
        "    def bin_numeric(v):\n",
        "        if v <= q[0]: return 0\n",
        "        if v <= q[1]: return 1\n",
        "        return 2\n",
        "    y = df[\"side_effect_severity\"].apply(bin_numeric).astype(int).values\n",
        "else:\n",
        "    approx = df[\"side_effect_severity\"].apply(to_three_bins).astype(float)\n",
        "    q = np.quantile(approx, [0.33, 0.66])\n",
        "    def bin_numeric(v):\n",
        "        if v <= q[0]: return 0\n",
        "        if v <= q[1]: return 1\n",
        "        return 2\n",
        "    y = approx.apply(bin_numeric).astype(int).values\n",
        "\n",
        "num_classes = 3\n",
        "class_names = [\"low\", \"moderate\", \"high\"]\n",
        "\n",
        "text_cols = [\"indications\", \"side_effects\", \"contraindications\", \"warnings\"]\n",
        "num_cols = [\"dosage_mg\", \"price_usd\", \"approval_year\"]\n",
        "cat_cols = [\"drug_class\", \"administration_route\", \"approval_status\", \"manufacturer\"]\n",
        "\n",
        "text_cols = [c for c in text_cols if c in df.columns]\n",
        "num_cols = [c for c in num_cols if c in df.columns]\n",
        "cat_cols = [c for c in cat_cols if c in df.columns]\n",
        "\n",
        "for c in text_cols:\n",
        "    df[c] = df[c].fillna(\"\")\n",
        "\n",
        "X_text = df[text_cols].apply(lambda r: \" \".join([str(v) for v in r.values]), axis=1)\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2), min_df=2)\n",
        "X_text_mat = tfidf.fit_transform(X_text)\n",
        "\n",
        "X_num = df[num_cols].fillna(df[num_cols].median()) if num_cols else pd.DataFrame(index=df.index)\n",
        "X_cat = df[cat_cols].fillna(\"UNK\") if cat_cols else pd.DataFrame(index=df.index)\n",
        "\n",
        "if not X_num.empty:\n",
        "    scaler = StandardScaler()\n",
        "    X_num_scaled = scaler.fit_transform(X_num.values)\n",
        "else:\n",
        "    X_num_scaled = np.zeros((len(df), 0))\n",
        "\n",
        "if not X_cat.empty:\n",
        "    X_cat_dummies = pd.get_dummies(X_cat, drop_first=True, dtype=np.float32)\n",
        "else:\n",
        "    X_cat_dummies = pd.DataFrame(index=df.index)\n",
        "\n",
        "from scipy import sparse\n",
        "X_other = np.hstack([X_num_scaled, X_cat_dummies.values]) if X_cat_dummies.shape[1] > 0 else X_num_scaled\n",
        "X = sparse.hstack([X_text_mat, sparse.csr_matrix(X_other)], format=\"csr\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
        ")\n",
        "\n",
        "R = np.array([\n",
        "    [ 1.0,      -0.2,   -0.5],\n",
        "    [-0.2,       1.0,   -0.2],\n",
        "    [-2.0,      -0.5,    1.2],\n",
        "])\n",
        "\n",
        "def batch_csr_to_torch(X_csr):\n",
        "    X_coo = X_csr.tocoo()\n",
        "    indices = torch.tensor(np.vstack((X_coo.row, X_coo.col)), dtype=torch.long)\n",
        "    values = torch.tensor(X_coo.data, dtype=torch.float32)\n",
        "    shape = torch.Size(X_coo.shape)\n",
        "    return torch.sparse_coo_tensor(indices, values, shape).to_dense()\n",
        "\n",
        "def sample_minibatch(X_csr, y_arr, batch_size=64):\n",
        "    idx = np.random.randint(0, X_csr.shape[0], size=batch_size)\n",
        "    Xb = X_csr[idx]\n",
        "    yb = y_arr[idx]\n",
        "    return batch_csr_to_torch(Xb), torch.tensor(yb, dtype=torch.long)\n",
        "\n",
        "def compute_reward(y_true, a):\n",
        "    return R[y_true, a]\n",
        "\n",
        "def evaluate_policy(pred, y_true):\n",
        "    cm = confusion_matrix(y_true, pred, labels=[0,1,2])\n",
        "    report = classification_report(y_true, pred, target_names=class_names, output_dict=True)\n",
        "    return cm, report\n",
        "\n",
        "class EpsilonGreedyBandit:\n",
        "    def __init__(self, n_actions=3, eps=0.1):\n",
        "        self.nA = n_actions\n",
        "        self.eps = eps\n",
        "        self.counts = np.zeros(n_actions, dtype=int)\n",
        "        self.values = np.zeros(n_actions, dtype=float)\n",
        "\n",
        "    def select(self):\n",
        "        if np.random.rand() < self.eps:\n",
        "            return np.random.randint(self.nA)\n",
        "        return int(np.argmax(self.values))\n",
        "\n",
        "    def update(self, a, r):\n",
        "        self.counts[a] += 1\n",
        "        n = self.counts[a]\n",
        "        self.values[a] += (r - self.values[a]) / n\n",
        "\n",
        "class UCB1Bandit:\n",
        "    def __init__(self, n_actions=3):\n",
        "        self.nA = n_actions\n",
        "        self.counts = np.zeros(n_actions, dtype=int)\n",
        "        self.values = np.zeros(n_actions, dtype=float)\n",
        "        self.t = 0\n",
        "\n",
        "    def select(self):\n",
        "        self.t += 1\n",
        "        for a in range(self.nA):\n",
        "            if self.counts[a] == 0:\n",
        "                return a\n",
        "        ucb = self.values + np.sqrt(2*np.log(self.t)/self.counts)\n",
        "        return int(np.argmax(ucb))\n",
        "\n",
        "    def update(self, a, r):\n",
        "        self.counts[a] += 1\n",
        "        n = self.counts[a]\n",
        "        self.values[a] += (r - self.values[a]) / n\n",
        "\n",
        "class ThompsonBandit:\n",
        "    def __init__(self, n_actions=3):\n",
        "        self.nA = n_actions\n",
        "        self.mu = np.zeros(n_actions)\n",
        "        self.lambda_prec = np.ones(n_actions)  # precision\n",
        "        self.tau = 1.0\n",
        "\n",
        "    def select(self):\n",
        "        samples = np.random.normal(self.mu, 1.0/np.sqrt(self.lambda_prec))\n",
        "        return int(np.argmax(samples))\n",
        "\n",
        "    def update(self, a, r):\n",
        "        self.lambda_prec[a] += self.tau\n",
        "        self.mu[a] = (self.mu[a]*(self.lambda_prec[a]-self.tau) + r) / self.lambda_prec[a]\n",
        "\n",
        "class LinUCB:\n",
        "    def __init__(self, d, n_actions=3, alpha=1.0, l2=1.0):\n",
        "        self.nA = n_actions\n",
        "        self.alpha = alpha\n",
        "        self.A = [l2 * np.eye(d) for _ in range(n_actions)]\n",
        "        self.b = [np.zeros((d,)) for _ in range(n_actions)]\n",
        "\n",
        "    def select(self, x):\n",
        "        p = np.zeros(self.nA)\n",
        "        for a in range(self.nA):\n",
        "            A_inv = np.linalg.inv(self.A[a])\n",
        "            theta = A_inv @ self.b[a]\n",
        "            p[a] = theta @ x + self.alpha * np.sqrt(x @ A_inv @ x)\n",
        "        return int(np.argmax(p))\n",
        "\n",
        "    def update(self, x, a, r):\n",
        "        self.A[a] += np.outer(x, x)\n",
        "        self.b[a] += r * x\n",
        "\n",
        "class LogisticTS:\n",
        "    def __init__(self, d, n_actions=3, l2=1.0):\n",
        "        self.nA = n_actions\n",
        "        self.d = d\n",
        "        self.l2 = l2\n",
        "        self.W = np.zeros((n_actions, d))\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        return 1/(1+np.exp(-z))\n",
        "\n",
        "    def select(self, x):\n",
        "        noise = np.random.normal(0, 0.1, size=self.W.shape)\n",
        "        W_s = self.W + noise\n",
        "        logits = W_s @ x\n",
        "        return int(np.argmax(logits))\n",
        "\n",
        "    def update(self, x, a, r):\n",
        "        y = 1 if r > 0 else 0\n",
        "        z = self.W[a] @ x\n",
        "        p = self._sigmoid(z)\n",
        "        grad = (y - p) * x - self.l2 * self.W[a]\n",
        "        self.W[a] += 0.05 * grad\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, hidden=256):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, out_dim),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, in_dim, n_actions, gamma=0.99, lr=1e-3, eps=0.1):\n",
        "        self.q = MLP(in_dim, n_actions, hidden=256)\n",
        "        self.target = MLP(in_dim, n_actions, hidden=256)\n",
        "        self.target.load_state_dict(self.q.state_dict())\n",
        "        self.gamma = gamma\n",
        "        self.optim = optim.Adam(self.q.parameters(), lr=lr)\n",
        "        self.eps = eps\n",
        "        self.nA = n_actions\n",
        "        self.losses = []\n",
        "\n",
        "    def act(self, x):\n",
        "        if np.random.rand() < self.eps:\n",
        "            return np.random.randint(self.nA)\n",
        "        with torch.no_grad():\n",
        "            q = self.q(x)\n",
        "            return int(torch.argmax(q, dim=-1).item())\n",
        "\n",
        "    def update(self, x, a, r, xn, done):\n",
        "        q = self.q(x)[0, a]\n",
        "        with torch.no_grad():\n",
        "            qn = self.target(xn).max(dim=-1).values\n",
        "            y = r + (0 if done else self.gamma * qn)\n",
        "        loss = F.mse_loss(q, y)\n",
        "        self.optim.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optim.step()\n",
        "        self.losses.append(loss.item())\n",
        "\n",
        "    def soft_update(self, tau=0.01):\n",
        "        for t, s in zip(self.target.parameters(), self.q.parameters()):\n",
        "            t.data.copy_((1 - tau) * t.data + tau * s.data)\n",
        "\n",
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self, in_dim, n_actions, hidden=256):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, n_actions),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.net(x), dim=-1)\n",
        "\n",
        "class ValueNet(nn.Module):\n",
        "    def __init__(self, in_dim, hidden=256):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class REINFORCEAgent:\n",
        "    def __init__(self, in_dim, n_actions, lr=1e-3):\n",
        "        self.policy = PolicyNet(in_dim, n_actions)\n",
        "        self.optim = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "\n",
        "    def act(self, x):\n",
        "        with torch.no_grad():\n",
        "            logp = self.policy(x)\n",
        "            p = torch.exp(logp)\n",
        "            a = torch.multinomial(p, num_samples=1)\n",
        "            return int(a.item())\n",
        "\n",
        "    def update(self, x, a, G):\n",
        "        logp = self.policy(x)[0, a]\n",
        "        loss = -logp * G\n",
        "        self.optim.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optim.step()\n",
        "\n",
        "class A2CAgent:\n",
        "    def __init__(self, in_dim, n_actions, lr=1e-3, gamma=0.99):\n",
        "        self.policy = PolicyNet(in_dim, n_actions)\n",
        "        self.value = ValueNet(in_dim)\n",
        "        self.op = optim.Adam(list(self.policy.parameters()) + list(self.value.parameters()), lr=lr)\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def act(self, x):\n",
        "        with torch.no_grad():\n",
        "            logp = self.policy(x)\n",
        "            p = torch.exp(logp)\n",
        "            a = torch.multinomial(p, num_samples=1)\n",
        "            return int(a.item())\n",
        "\n",
        "    def update(self, x, a, r, xn, done):\n",
        "        V = self.value(x)\n",
        "        with torch.no_grad():\n",
        "            Vn = self.value(xn)\n",
        "            target = r + (0 if done else self.gamma * Vn)\n",
        "            adv = target - V\n",
        "        logp = self.policy(x)[0, a]\n",
        "        actor_loss = -logp * adv.detach()\n",
        "        critic_loss = F.mse_loss(V, target)\n",
        "        loss = actor_loss + critic_loss\n",
        "        self.op.zero_grad()\n",
        "        loss.backward()\n",
        "        self.op.step()\n",
        "\n",
        "class BehaviorCloning:\n",
        "    def __init__(self, in_dim, n_actions, lr=1e-3):\n",
        "        self.net = MLP(in_dim, n_actions)\n",
        "        self.op = optim.Adam(self.net.parameters(), lr=lr)\n",
        "\n",
        "    def fit(self, X_t, y_t, steps=500, bs=64):\n",
        "        N = X_t.shape[0]\n",
        "        X_dense = batch_csr_to_torch(X_t)\n",
        "        y_t = torch.tensor(y_t, dtype=torch.long)\n",
        "        for _ in range(steps):\n",
        "            idx = np.random.randint(0, N, size=bs)\n",
        "            xb = X_dense[idx]\n",
        "            yb = y_t[idx]\n",
        "            logits = self.net(xb)\n",
        "            loss = F.cross_entropy(logits, yb)\n",
        "            self.op.zero_grad()\n",
        "            loss.backward()\n",
        "            self.op.step()\n",
        "        return self\n",
        "\n",
        "    def predict(self, X_csr):\n",
        "        with torch.no_grad():\n",
        "            Xd = batch_csr_to_torch(X_csr)\n",
        "            logits = self.net(Xd)\n",
        "            return logits.argmax(dim=-1).cpu().numpy()\n",
        "\n",
        "class CQLlite(DQNAgent):\n",
        "    def update(self, x, a, r, xn, done):\n",
        "        q_all = self.q(x)\n",
        "        q = q_all[0, a]\n",
        "        with torch.no_grad():\n",
        "            qn = self.target(xn).max(dim=-1).values\n",
        "            y = r + (0 if done else self.gamma * qn)\n",
        "        cql_penalty = 1e-3 * (q_all.pow(2).mean())\n",
        "        loss = F.mse_loss(q, y) + cql_penalty\n",
        "        self.optim.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optim.step()\n",
        "\n",
        "# Add placeholder functions\n",
        "def train_non_contextual_bandits():\n",
        "    print(\"Placeholder for train_non_contextual_bandits\")\n",
        "    return {}\n",
        "\n",
        "def train_contextual_bandits():\n",
        "    print(\"Placeholder for train_contextual_bandits\")\n",
        "    return {}\n",
        "\n",
        "def train_deep_rl():\n",
        "    print(\"Placeholder for train_deep_rl\")\n",
        "    # Return dummy values for now\n",
        "    return {}, None, None, None\n",
        "\n",
        "def train_offline_rl():\n",
        "    print(\"Placeholder for train_offline_rl\")\n",
        "    return {}\n",
        "\n",
        "def explain_policy_with_shap(policy, X_train, X_test):\n",
        "    print(\"Placeholder for explain_policy_with_shap\")\n",
        "    # Return dummy values for now\n",
        "    return None, None\n",
        "\n",
        "def distill_policy_to_tree(policy, X_train, y_train):\n",
        "    print(\"Placeholder for distill_policy_to_tree\")\n",
        "    # Return dummy values for now\n",
        "    return None, None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    summary = {}\n",
        "\n",
        "    summary[\"non_contextual_bandits\"] = train_non_contextual_bandits()\n",
        "    summary[\"contextual_bandits\"] = train_contextual_bandits()\n",
        "\n",
        "    deep_results, policy_for_xai, Xtr_d, Xte_d = train_deep_rl()\n",
        "    summary[\"deep_rl\"] = deep_results\n",
        "\n",
        "    summary[\"offline_rl\"] = train_offline_rl()\n",
        "\n",
        "    try:\n",
        "        shap_vals, shap_x = explain_policy_with_shap(policy_for_xai, Xtr_d, Xte_d)\n",
        "        summary[\"shap_saved\"] = SHAP_AVAILABLE\n",
        "    except Exception as e:\n",
        "        summary[\"shap_error\"] = str(e)\n",
        "\n",
        "    try:\n",
        "        tree, rules = distill_policy_to_tree(policy_for_xai, X_train, y_train)\n",
        "        summary[\"surrogate_tree\"] = \"ok\"\n",
        "    except Exception as e:\n",
        "        summary[\"surrogate_tree_error\"] = str(e)\n",
        "\n",
        "    with open(os.path.join(OUTDIR, \"RUN_SUMMARY.json\"), \"w\") as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "    print(\"Done. Artifacts in:\", OUTDIR)"
      ]
    }
  ]
}